{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc31c064",
   "metadata": {},
   "source": [
    "# Veolia Data Science Internship &mdash; Technical Assignment\n",
    "\n",
    "You are asked to work on the data of a wastewater treatment plant located in Southeast Asia.\n",
    "\n",
    "First, I advise you to create a local environment using Python 3.11, and the packages you will find in `requirements.txt`.\n",
    "\n",
    "With Conda, you can follow these steps (adapt for Python venv if needed):\n",
    "\n",
    "```bash\n",
    "conda create -n env_assignment python=3.11\n",
    "conda activate env_assignment\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0729c0",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "A colleague from the office of Veolia in Southeast Asia just sent you an email, asking for your help. \n",
    "\n",
    "She is in charge of a wastewater treatment plant, and would like you to explore the possibility of developping a tool that forecasts the volume of wastewater entering the plant every hour.\n",
    "\n",
    "The objective here is to see if it is feasable, in which case you will showcase your proof of concept to this colleague.\n",
    "\n",
    "You are provided with the following dataset, as is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46012cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils.data import read_data\n",
    "\n",
    "df = read_data()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2ecb34",
   "metadata": {},
   "source": [
    "_ww_ means _wastewater_, if you had not guessed ;)\n",
    "\n",
    "### Variables\n",
    "\n",
    "| Variable Name                        | Type       | Unit         | Description                                                                                      |\n",
    "|---------------------------------------|------------|--------------|--------------------------------------------------------------------------------------------------|\n",
    "| wastewater volume                    | timeseries | m¬≥       | **Target variable**. Total hourly volume of water entering the WWTP.                             |\n",
    "| ww line 1                   | timeseries | m¬≥       | Hourly volume of wastewater (pipe 1).                               |\n",
    "| ww line 2                   | timeseries | m¬≥       | Hourly volume of wastewater (pipe 2).                                     |\n",
    "| ww line 3                  | timeseries | m¬≥       | Hourly volume of wastewater (pipe 3).                    |\n",
    "| ww line 4      | timeseries | m¬≥           | Hourly volume of wastewater (pipe 4).                                    |\n",
    "| rainfall                              | timeseries | mm      | Hourly rainfall amount measured on site.                                                                          |\n",
    "| temperature_2m                        | timeseries | ¬∞C           | Air temperature at 2 meters above ground.                                                        |\n",
    "| relative_humidity_2m                  | timeseries | %            | Relative humidity at 2 meters above ground.                                                      |\n",
    "| precipitation                         | timeseries | mm           | Total precipitation (rain, showers, snow) in the preceding hour.                                 |\n",
    "| rain                                  | timeseries | mm           | Only liquid precipitation (rain, showers) in the preceding hour.                                 |\n",
    "\n",
    "\n",
    "#### Explanations\n",
    "\n",
    "* The **wastewater volume** is the sum of four main inlet flows: ww lines 1, 2, 3 and 4. They are different type of wastewater lines, coming from different places.\n",
    "* **rainfall** is measured on site by a sensor. \n",
    "* The other weather variables (four last lines of the table above) are gathered from a weather provider for the broad area of the plant, and are considered predictions. You may use those, and **only those**, for your wastewater forecast. **rainfall** or any water volume are _unknown_ for tduring your forecast would be a data leak.\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79653afa",
   "metadata": {},
   "source": [
    "## ü§ñ Before You Start: A Note on AI Usage\n",
    "\n",
    "I know many of you will ask an AI for help with this assignment. **That's perfectly fine** - AI is a powerful tool that you should learn to use efficiently. However, you should develop **critical thinking skills** about AI outputs.\n",
    "\n",
    "### Some tasks to help you with that, and to see your reasoning:\n",
    "\n",
    "1. If it is your way of working, **ask your favorite LLM** how to approach this assignment.\n",
    "2. **Paste your initial conversation below** and then work as you would have done normally.\n",
    "3. **At the end of your work**, identify what the AI initially got right, what it missed, and what might have been problematic (if anything) if you had strictly followed this path.\n",
    "\n",
    "\n",
    "**Remember**: The goal is not to avoid AI, but to use it wisely. \n",
    "\n",
    "If you do not use AI, please mention it while still doing these steps for you to be able to review the initial AI output at the end of your assignment. If you have any comment, also feel free to elaborate on that below.\n",
    "\n",
    "\n",
    "### üìù Paste your AI conversation here:\n",
    "\n",
    "*[Your AI query and response go here]*\n",
    "\n",
    "### ü§î Your critical analysis:\n",
    "\n",
    "Throughout the project, I used ChatGPT as a complementary tool rather than a decision-maker. The core reasoning, modelling choices, and interpretation of results came from my own analysis, but ChatGPT was helpful for accelerating specific tasks such as clarifying certain Python behaviors, validating the intuition behind a methodological choice, or reformulating early ideas more clearly. I mainly relied on it the way one would consult a knowledgeable colleague‚Äîby asking targeted, technical questions rather than outsourcing the thinking. For example, at one point I used a prompt such as ‚ÄúGiven a 7-day sliding window of learned representations, what are the pros and cons of using a simple linear head versus a tree-based model for 24-hour forecasting?‚Äù This type of interaction helped me refine my approach without influencing its direction, ensuring that the conceptual and modelling work remained my own.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434a5f5c",
   "metadata": {},
   "source": [
    "## Your Work Starts Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c95f1c6",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color: rgb(36, 117, 179); font-size:130%; text-align:left\">\n",
    "\n",
    "### **Introduction & approach summary**\n",
    "\n",
    "The first long task was to understand the dataset and the different relationships between the variables (very complex between correlation, autocorrelation, and multiple seasonal dependencies). \n",
    "\n",
    "My work was organized by going back and forth between tests and research, while also leaving time between sessions to take a step back from the problem and try to better understand its depth. \n",
    "\n",
    "After analyzing the data, I chose to test a simple initial model, which would give me some initial feedback on my analyses, then I tried to improve it using the results and other ideas about bias that came to me later. \n",
    "\n",
    "Finally, I wanted to spend some time looking for a model that would better match the complexity of the problem, namely a multivariate variable to predict (the sum of four variables with distinct temporal behaviors). While researching deep learning, I came across an article that addressed this type of multi-scale problem and also provided an example of implementation. In the end, this was the model that seemed to work best.\n",
    "\n",
    "Below is a summary of the analyses carried out.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e934110",
   "metadata": {},
   "source": [
    "<a id=\"contents_tabel\"></a>    \n",
    "<div style=\"border-radius:10px; padding: 15px; background-color: rgb(36, 117, 179); font-size:130%; text-align:left\">\n",
    "\n",
    "<h2 align=\"left\"><font>Table of Contents:</font></h2>\n",
    "    \n",
    "* [Part I - Charging the librairies](#librairies)\n",
    "* [Part II - Initial Data Analysis](#initial_analysis) \n",
    "    - [II.1. Dataset Overview](#overview) \n",
    "    - [II.2. Summary Statistics](#statistics) \n",
    "    - [II.3. First vizualization of the data](#vizualization)\n",
    "    - [II.4. Checking Correlation and Auto Correlation](#Correlation)\n",
    "        - [Autocorrelation](#autocorrelation)\n",
    "        - [Correlation](#correlation)\n",
    "* [Part III - Data Cleaning & Transformation](#data_cleaning)\n",
    "    - [III.1. Handling Missing Values](#missing_values)\n",
    "    - [III.2. Handling Duplicates](#duplicates)\n",
    "    - [III.3. Handling Outliers](#outliers)\n",
    "* [Part IV - A First Simple Model, LightGBM](#LightGBM)\n",
    "    - [IV.1. Feature Enginerring for LightGBM](#FE_LGBM)\n",
    "        - [Part IV.1.A A New DataFrame](#new_dataframe) \n",
    "        - [Part IV.1.B Lags & rolling_means](#lags)\n",
    "    - [Part IV.2 Implementing the model](#LGBM)\n",
    "        - [Part IV.2.A Defining the target and feature selection](#target)\n",
    "        - [Part IV.2.B Splitting the Data](#split)\n",
    "        - [Part IV.1.C Finding the hyperparameters](#hyperparamters) \n",
    "        - [Part IV.1.D Model and Evaluation](#model) \n",
    "        - [Part IV.1.E Feature Importance](#feature)\n",
    "* [PART V - Improving the LightGBM](#v2)\n",
    "    - [Part V.1 Feature Enginerring for LightGBM v.2](#fe_2)\n",
    "        - [Part V.1.A Target and lags](#lags2)\n",
    "        - [Part V.1.B Autocorrelation based features](correlation2) \n",
    "        - [Part V.1.C Cyclical features](#cyclical) \n",
    "    - [Part V.2 Implementing the model](#model2)\n",
    "        - [Part V.2.A Train/test split by shuffled 7-day blocks](#split2)\n",
    "        - [Part V.2.B Splitting the Data](#kmeans_model)\n",
    "        - [Part V.2.C Finding the hyperparameters](#evaluation)\n",
    "        - [Part V.2.D Model and Evaluation](#evalutaion2)\n",
    "* [PART VI - Testing a more advanced model using Deep Learning - Ts2Vec](#ts2vec)\n",
    "    - [Part VI.1 Preparing the Data](#cluster_distributuion) \n",
    "    - [Part VI.2 Implementing the model](#evaluations_metrics)\n",
    "        - [Part VI.2.A Training ts2vec](#profiling)\n",
    "        - [Part VI.2.B Linear regression on the encoded train set](#radar_chart)\n",
    "        - [Part VI.2.C Evaluting the model](#model3)\n",
    "        - [Part VI.2.D Recreating the wastewater volume feature to plot the results](#plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cc05c7",
   "metadata": {},
   "source": [
    "<a id=\"librairies\"></a>\n",
    "\n",
    "# <p style=\"color:#E71B24; font-family:'Futura', 'Century Gothic', 'Verdana', sans-serif; font-weight:600; letter-spacing:0.5px; text-align:center;\">PART I - Charging the libraries</p>\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "86039ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from lightgbm import LGBMRegressor\n",
    "import optuna\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from ts2vec import TS2Vec\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0476a998",
   "metadata": {},
   "source": [
    "<p style=\"color:red\"> xx mettre des commentaires expliquant √† quoi servent chacune des librairies </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0a4a62",
   "metadata": {},
   "source": [
    "<a id=\"initial_analysis\"></a>\n",
    "# <p style=\"color:#E71B24; font-family:'Futura', 'Century Gothic', 'Verdana', sans-serif; font-weight:600; letter-spacing:0.5px; text-align:center;\">PART II - First analysis on the data</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2c101b",
   "metadata": {},
   "source": [
    "<a id=\"overview\"></a>\n",
    "## <p style=\"color: rgb(300, 110, 110); font-family:'Futura', 'Century Gothic', 'Verdana', sans-serif; font-weight:600; letter-spacing:0.5px;\">  Part II.1 Dataset Overview</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb107f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c8b244",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color:rgb(36, 117, 179); font-size:120%; text-align:left\">\n",
    "\n",
    "**Inferences :** \n",
    "\n",
    "- The dataset contains 25727 entries, covering the 2018-2021 period and each entry corresponding to an hour of day during these 3 years. It is important to check if there are any gaps in the period.\n",
    "\n",
    "\n",
    "- All the columns are of object type, except the rainfall one which is a float type. As they actually are all numbers, we will change these types to float, which will be easier to deal with.\n",
    "Some columns (WW volume, WW line 4 and relative_humidity_2m) contain missing values which we will address later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26fa800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for eventual gaps in the time series\n",
    "time_diffs = df.index.to_series().diff()\n",
    "gaps = time_diffs[time_diffs > pd.Timedelta(hours=1)]\n",
    "\n",
    "if gaps.empty:\n",
    "    print(\"No gaps found in the time series.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7013be55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the dtypes of the columns to float type, taking care of some eventual commas as decimal separators\n",
    "for col in df.columns:\n",
    "    if col != \"date\":\n",
    "        df[col] = (\n",
    "            df[col]\n",
    "            .astype(str)                  # converting to string first\n",
    "            .str.replace(',', '.', regex=False)  # replacing commas with dots\n",
    "            .astype(float)                # converting to float\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e15684a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05479694",
   "metadata": {},
   "source": [
    "<a id=\"statistics\"></a>\n",
    "## <p style=\"color: rgb(300, 110, 110); font-family:'Futura', 'Century Gothic', 'Verdana', sans-serif; font-weight:600; letter-spacing:0.5px;\">Part II.2 Some First Statistics on the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298f5a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ecb410",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color:rgb(36, 117, 179); font-size:120%; text-align:left\">\n",
    "\n",
    "#### Inferences :\n",
    "\n",
    "**WW Volume :**  \n",
    "- The mean ww volume entering the WWTP is 1535 with a large std of 955.5.  \n",
    "- It has a wide range with a maximum at 14312, much higher than the 75% quartile, and which we will thus have to further investigate.  \n",
    "- The 7 seven missing values will also be treated. \n",
    "\n",
    "**WW lines 1,2,3,4:**  \n",
    "- Same inferences than the WW Volume's column.  \n",
    "- The minimum values are not below 0, so there are no lower outliers, but the maximum values will need to be investigated.  \n",
    "\n",
    "**Rainfall:**  \n",
    "- Given the 25%, 50% and 75% quartiles, we can observe that the entries of this column are mainly zero.   \n",
    "- The maximal value once again seems very big, but not physically impossible during a particularly stormy day.\n",
    "\n",
    "**temperature_2m and relative_humidity_2m:**  \n",
    "- These columns do not seem to have strange values (physically coherent), excepted for de 103% of relative humidity. It can however just a sensor's mistake due to noise as it is not that far from a coherent value. \n",
    "- The missing values of the relative humidity column will have to be addressed.\n",
    "\n",
    "**rain and precipitation:**\n",
    "- These columns seem to be close to identical, which is coherent considering they only differ on the snow counting. It will be further investigated wether it is useful to keep the two columns while applying the models. \n",
    "- We can also observe that the mean is very close to the rainfall column, which seems to indicate that the weather predications are rather accurate on average. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5569c816",
   "metadata": {},
   "source": [
    "<a id=\"vizualization\"></a>\n",
    "## <p style=\"color: rgb(300, 110, 110); font-family:'Futura', 'Century Gothic', 'Verdana', sans-serif; font-weight:600; letter-spacing:0.5px;\"> Part II.3 First vizualization of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdabf956",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color:rgb(36, 117, 179); font-size:120%; text-align:left\">In this section, we will take a first look at the data by testing different views (hourly, daily, weekly, montly). The goal is to have a first idea of the variations of the wastewater volume, its trends, and eventual seasonality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30ba2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the wastewater volume over the entire time period\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(df.index, df['wastewater volume'], label='Wastewater Volume (m¬≥)', linewidth=1.5)\n",
    "plt.title('Hourly variations in wastewater volume')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Volume (m¬≥)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a675e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the wastewater volume over Nadays days for better clarity\n",
    "i=1000\n",
    "Ndays=5\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(df.index[i:24*Ndays+i], df['wastewater volume'][i:24*Ndays+i], label='Wastewater Volume (m¬≥)', linewidth=1.5)\n",
    "plt.title('Hourly variations in wastewater volume')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Volume (m¬≥)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5d6b20",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color:rgb(36, 117, 179); font-size:120%; text-align:left\">\n",
    "\n",
    "#### Inferences :\n",
    "\n",
    "**Significant variability**  \n",
    "Wastewater volume varies a little from hour to hour, with frequent peaks and periods of low activity.\n",
    "Peaks can exceed 10,000 m¬≥, while troughs often fall below 2,000 m¬≥.\n",
    "\n",
    "**Apparent seasonality**  \n",
    "Repeating patterns are observed on regular cycles, suggesting daily or weekly seasonality.\n",
    "Peaks seem to occur at similar times of day, probably linked to human activity (e.g., morning and evening).\n",
    "\n",
    "**Long-term trend**  \n",
    "There is no clear upward or downward trend over the entire period, but seasonal variations are noted\n",
    "\n",
    "**Outliers**  \n",
    "Some values are well above the average, which may indicate exceptional events (heavy rainfall, industrial incidents, or errors).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2171f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean = df.resample('d').mean()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(df_mean.index, df_mean['wastewater volume'], label='Wastewater Volume (m¬≥)', linewidth=1.5)\n",
    "plt.title('Daily changes in wastewater volume')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Volume (m¬≥)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b15913b",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color:rgb(36, 117, 179); font-size:120%; text-align:left\">\n",
    "Daily variability\n",
    "\n",
    "The average daily volume of wastewater generally varies between 1,000 m¬≥ and 4,000 m¬≥, with peaks of up to 7,000 m¬≥.\n",
    "Peaks are less frequent and smaller than in the hourly graph, but remain significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade83435",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean = df.resample('W').mean()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(df_mean.index, df_mean['wastewater volume'], label='Wastewater Volume (m¬≥)', linewidth=1.5)\n",
    "plt.title('Weekly changes in wastewater volume')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Volume (m¬≥)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e44d46f",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color:rgb(36, 117, 179); font-size:120%; text-align:left\">\n",
    "Weekly variability  \n",
    " \n",
    "Here we can see that the peaks are much less significant (maximum at 2600), which shows that large variations are very occasional (and almost entirely smoothed out over a week)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e65493",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean = df.resample('M').mean()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(df_mean.index, df_mean['wastewater volume'], label='Wastewater Volume (m¬≥)', linewidth=1.5)\n",
    "plt.title('Monthly changes in wastewater volume')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Volume (m¬≥)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a7000f",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color:rgb(36, 117, 179); font-size:120%; text-align:left\">There appears to be annual seasonality, with recurring peaks at certain times of the year.\n",
    "For example, peaks are observed in summer (June-September) and lows in winter (December-January). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d31994",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['relative_humidity_2m', 'rain', 'temperature_2m']\n",
    "titles = [\n",
    "    'Relative Humidity %)',\n",
    "    'Rainfall (mm/h)',\n",
    "    'Air Temperature at 2m (¬∞C)'\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(len(variables), 1, figsize=(12, 6), sharex=True)\n",
    "\n",
    "for i, var in enumerate(variables):\n",
    "    axes[i].plot(df.index, df[var], linewidth=1.2, color='tab:blue')\n",
    "    axes[i].set_title(titles[i], fontsize=11)\n",
    "    axes[i].grid(True)\n",
    "    axes[i].set_ylabel(var)\n",
    "\n",
    "axes[-1].set_xlabel('Time')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9626ae8f",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color:rgb(36, 117, 179); font-size:120%; text-align:left\">\n",
    "Here we can see that the three variables have variations that appear to be seasonal, which was excepted for weather features\n",
    "\n",
    "Their peaks also appear to be coordinated, which could indicate a correlation between them, which then again seems logical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf77a2f",
   "metadata": {},
   "source": [
    "<a id=\"Correlation\"></a>\n",
    "## <p style=\"color: rgb(300, 110, 110); font-family:'Futura', 'Century Gothic', 'Verdana', sans-serif; font-weight:600; letter-spacing:0.5px;\"> Part II.4 Checking Correlation and Auto Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfcb9a0",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color:rgb(36, 117, 179); font-size:120%; text-align:left\">\n",
    "\n",
    "In this setion we continue to investigate the data set. We take a better look at the temporal dependencies within our dataset by examining the Autocorrelation Function (ACF) and the Partial Autocorrelation Function (PACF).\n",
    "\n",
    "\n",
    "- **ATF :** measures how a time-series variable is correlated with its own past values across different lags. It helps reveal overall patterns such as persistence or repeating cycles.\n",
    "\n",
    "- **PACF :** isolates the direct relationship between a variable and its past values at a specific lag, removing the effects of the intermediate lags. It is especially useful for identifying the appropriate number of autoregressive terms in time-series models.\n",
    "\n",
    " These tools help us identify patterns such as persistence, seasonality, and lagged relationships, which are essential for understanding the underlying dynamics of each variable and for guiding subsequent time-series modeling steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea52960",
   "metadata": {},
   "source": [
    "<a id=\"autocorrelation\"></a>\n",
    "### <p style=\"color: rgb(250, 150, 150); font-family:'Futura', 'Century Gothic', 'Verdana', sans-serif; font-weight:600; letter-spacing:0.5px;\"> II.4.A Autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a357d401",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = df.columns.tolist()\n",
    "n_vars = len(variables)\n",
    "fig, axes = plt.subplots(n_vars, 2, figsize=(12, n_vars * 3))  # 3 inches height per variable\n",
    "\n",
    "for i, var in enumerate(variables):\n",
    "    plot_acf(df[var].dropna(), ax=axes[i, 0], lags=48, title=f\"ACF - {var}\")\n",
    "    plot_pacf(df[var].dropna(), ax=axes[i, 1], lags=48, title=f\"PACF - {var}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c6289c",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color:rgb(36, 117, 179); font-size:120%; text-align:left\">\n",
    "\n",
    "**ACF**\n",
    "\n",
    "- The ww lines 2-3 see their ACF decrease slightly and then fluctuate. This therefore shows cyclical autocorrelation, with high peaks occuring at lag 12,24 and 48, while low peaks occur at lags 6,18,30 etc\n",
    "- The rain, rainfall, precipation and ww line have a slowly deacreasing ACF, suggesting that they are influenced by the 10-15 values coming before but much \n",
    "less by the further ones. \n",
    "\n",
    "**PCF**\n",
    "\n",
    "- All the variables have a first (and sometimes second) big peak. \n",
    "- The values of the other lags are close to zero, indicating that short-term dependence is mainly captured by the first lag.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210d6a3c",
   "metadata": {},
   "source": [
    "<a id=\"correlation\"></a>\n",
    "### <p style=\"color: rgb(250, 150, 150); font-family:'Futura', 'Century Gothic', 'Verdana', sans-serif; font-weight:600; letter-spacing:0.5px;\"> II.4.B Autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae991dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chacking correlaion\n",
    "\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "plt.figure(figsize=(8,6))  # taille de la figure\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, fmt=\".2f\")\n",
    "plt.title(\"Matrice de Corr√©lation\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d962c2ae",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color:rgb(36, 117, 179); font-size:120%; text-align:left\">\n",
    "\n",
    "#### Inferences \n",
    "\n",
    "**Wastewater Volume:**\n",
    "\n",
    "Strong correlations with the four ww lines 1 as expected as it is thei sum.\n",
    "Positive correlation with rainfall (0.58), suggesting that rainfall impacts wastewater volume the most of the weather variables.\n",
    "\n",
    "\n",
    "\n",
    "**Wastewater Lines:**\n",
    "\n",
    "ww line 1 and ww line 2 are highly correlated (0.60), indicating a potential shared influence or similar behavior.\n",
    "ww line 3 and ww line 4 also show moderate correlations with other wastewater lines.\n",
    "\n",
    "\n",
    "\n",
    "**Meteorological Variables:**\n",
    "\n",
    "Rainfall shows a moderate positive correlation with wastewater volume (0.58) and various wastewater lines.\n",
    "Temperature_2m and relative_humidity_2m have low or negligible correlations with wastewater volume, indicating a weaker influence.\n",
    "Rain and precipitation are perfectly correlated (1.00), as expected, since they almost represent the same thing. \n",
    "\n",
    "\n",
    "It could be interesting to see if other correlations appear with a daily average, as some variables seem to have daily autocorrelations (as seen just before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643ef6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking daily correlation\n",
    "\n",
    "daily_df = df.resample('D').mean()\n",
    "daily_correlation_matrix = daily_df.corr()\n",
    "\n",
    "plt.figure(figsize=(8,6))  # taille de la figure\n",
    "sns.heatmap(daily_correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, fmt=\".2f\")\n",
    "plt.title(\"Daily average correlation matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badcbd71",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color:rgb(36, 117, 179); font-size:120%; text-align:left\">\n",
    "\n",
    "#### Inferences\n",
    "\n",
    "\n",
    "**Wastewater Volume:**\n",
    "\n",
    "Extremely high positive correlation with ww line 1 (0.99) and ww line 2 (0.93), indicating that these lines are primary contributors to the overall wastewater volume.\n",
    "Strong positive correlation with ww line 4 (0.86) and ww line 3 (0.82).\n",
    "High positive correlation with rainfall (0.96), suggesting a significant impact of rainfall on wastewater volume.\n",
    "\n",
    "\n",
    "\n",
    "**Wastewater Lines:**\n",
    "\n",
    "ww line 1 and ww line 2 are highly correlated (0.89), indicating similar behavior or shared influences.\n",
    "ww line 3 and ww line 4 also show strong correlations with other wastewater lines, though slightly less than ww line 1 and ww line 2.\n",
    "\n",
    "\n",
    "\n",
    "**Meteorological Variables:**\n",
    "\n",
    "Rainfall shows a very high positive correlation with wastewater volume (0.96) and strong correlations with various wastewater lines, indicating its significant influence.\n",
    "Rain and precipitation are perfectly correlated (1.00), as they likely represent similar measurements.\n",
    "Temperature_2m and relative_humidity_2m have low correlations with wastewater volume, indicating a weaker influence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64416bc",
   "metadata": {},
   "source": [
    "<a id=\"data_cleaning\"></a>\n",
    "# <p style=\"color:#E71B24; font-family:'Futura', 'Century Gothic', 'Verdana', sans-serif; font-weight:600; letter-spacing:0.5px; text-align:center;\">PART III - Data Cleaning & Transformation</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f534def",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color:rgb(36, 117, 179); font-size:120%; text-align:left\">\n",
    "\n",
    "This step encompasses a comprehensive cleaning and transformation process to refine the dataset. It includes addressing missing values, eliminating duplicate entries, correcting anomalies, and other necessary adjustments to prepare the data for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35ea890",
   "metadata": {},
   "source": [
    "<a id=\"missing_values\"></a>\n",
    "## <p style=\"color: rgb(300, 110, 110); font-family:'Futura', 'Century Gothic', 'Verdana', sans-serif; font-weight:600; letter-spacing:0.5px;\"> Part III.1 Handling Missing Values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6551e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualising the missing values \n",
    "df[df.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416ca676",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color:rgb(36, 117, 179); font-size:120%; text-align:left\">\n",
    "\n",
    "- We can see that the rows where the wastewater volume is missing are the same as the ones where the data on the ww line 4 is missing too. We thus can't just sum or subtract the other values to get the one missing. We can see however that the missing values don't follow each other. \n",
    "\n",
    "- The missing values in the humidity column are also isolated. \n",
    "\n",
    "Depending on the model used in the end, even if the missing values only represent 0,0003% of the dataset, deleting them could confuse the model. As they don't follow each other, and because we saw in the previous analyses that the humidity, the ww volume and the ww4 are very influenced by the values just before, a **linear interpolation** seems the safest option.\n",
    "Since we can obtain the ww volume based on the four other ww, we are only going to do the linear interpolation on the humidty and the ww4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ef6ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear interpolation on the humidity and the ww4 columns to fill the missing values\n",
    "\n",
    "df['relative_humidity_2m'] = df['relative_humidity_2m'].interpolate(method='time')\n",
    "df['ww line 4'] = df['ww line 4'].interpolate(method='time')\n",
    "\n",
    "df['wastewater volume'] = df['wastewater volume'].fillna(df[['ww line 1','ww line 2','ww line 3','ww line 4']].sum(axis=1))\n",
    "\n",
    "df[df.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fd4bd3",
   "metadata": {},
   "source": [
    "<a id=\"duplicates\"></a>\n",
    "## <p style=\"color: rgb(300, 110, 110); font-family:'Futura', 'Century Gothic', 'Verdana', sans-serif; font-weight:600; letter-spacing:0.5px;\">Part III.2 Handling Duplicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e177a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking if there are any duplicated rows\n",
    "duplicates = df[df.duplicated(keep=False)]  # keep=False to see all the duplicated rows\n",
    "if duplicates.empty:\n",
    "    print(\"No duplicated rows found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3216abaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if there are any duplicated timestamps\n",
    "duplicated_timestamps = df.index[df.index.duplicated(keep=False)]\n",
    "if duplicated_timestamps.empty:\n",
    "    print(\"No duplicated timestamps found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb8448f",
   "metadata": {},
   "source": [
    "<a id=\"outliers\"></a>\n",
    "## <p style=\"color: rgb(300, 110, 110); font-family:'Futura', 'Century Gothic', 'Verdana', sans-serif; font-weight:600; letter-spacing:0.5px;\">Part III.3 Handling Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d93652",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color:rgb(36, 117, 179); font-size:120%; text-align:left\">\n",
    "\n",
    "The strategy here is to examine whether the abnormal values observed in a given variable are correlated with extreme values in the other columns.\n",
    "- If such correlations exist, the anomalies are likely meaningful patterns rather than errors, and may carry useful information for the model. We will not try to adress them in this section because the sensitivity to outliers depends on the model, and thy should thus be adressed accordingly. \n",
    "- If no correlation is found, the values are more likely to be genuine errors, in which case further inspection is recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c398fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['wastewater volume'] > 10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866decb4",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color:rgb(36, 117, 179); font-size:120%; text-align:left\">\n",
    "\n",
    "We can see that every row here has relatively high values for the weather variables, except for the fifth row. This row actually has a bigger issue : the wastewater volume is not the sum of the other four ww lines. Let's find out if there are others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec4c597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicit column list\n",
    "cols = ['ww line 1', 'ww line 2', 'ww line 3', 'ww line 4']\n",
    "total_col = 'wastewater volume'\n",
    "\n",
    "# Calculate the sum of the components\n",
    "components_sum = df[cols].sum(axis=1)\n",
    "\n",
    "# Boolean mask where the total is NOT equal to the sum of components\n",
    "mask = ~np.isclose(df[total_col], components_sum, atol=1e-1, rtol=1e-1)\n",
    "\n",
    "# Extract only the inconsistent rows\n",
    "mismatched_rows = df.loc[mask, df.columns].copy()\n",
    "\n",
    "# Add a difference column for clarity\n",
    "mismatched_rows['diff'] = df.loc[mask, total_col] - components_sum[mask]\n",
    "\n",
    "# Display results\n",
    "print(f\"Number of inconsistent rows: {mask.sum()}\")\n",
    "display(mismatched_rows.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d4c3fd",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color:rgb(36, 117, 179); font-size:120%; text-align:left\">Only two rows have this issue, but to know what to fix, we need to take a look at the rows coming just before and after each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde9e9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Timestamp cible\n",
    "timestamp_cible = pd.to_datetime(\"2019-06-04 03:00:00\")\n",
    "n = 5  # Nombre d'heures avant/apr√®s\n",
    "\n",
    "# Trouver l'index du timestamp cible\n",
    "index_cible = df.index.get_loc(timestamp_cible)\n",
    "\n",
    "# S√©lectionner les lignes\n",
    "start = max(0, index_cible - n)\n",
    "end = min(len(df), index_cible + n + 1)\n",
    "df_selection = df.iloc[start:end]\n",
    "\n",
    "df_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35eb8f72",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color:rgb(36, 117, 179); font-size:120%; text-align:left\">\n",
    "\n",
    "The outlier appears to be the wastewater volume, so it will be replaced by the sum of the other four values. Let's now take a look at the other problematic row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d134ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Timestamp cible\n",
    "timestamp_cible = pd.to_datetime(\"2019-07-07 14:00:00\")\n",
    "n = 5  # Nombre d'heures avant/apr√®s\n",
    "\n",
    "# Trouver l'index du timestamp cible\n",
    "index_cible = df.index.get_loc(timestamp_cible)\n",
    "\n",
    "# S√©lectionner les lignes\n",
    "start = max(0, index_cible - n)\n",
    "end = min(len(df), index_cible + n + 1)\n",
    "df_selection = df.iloc[start:end]\n",
    "\n",
    "df_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7605944d",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color:rgb(36, 117, 179); font-size:120%; text-align:left\">\n",
    "Here, it is the value of ww line 4 that seems out of bound. Once again, we will use the other 4 to deal with the problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "96c9f8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the wastewater volume and the ww line 4 values at the identified timestamps\n",
    "df.loc[\"2019-06-04 03:00:00\", \"wastewater volume\"] = df.loc[\"2019-06-04 03:00:00\", cols].sum() \n",
    "df.loc[\"2019-07-07 14:00:00\", \"ww line 4\"] = df.loc[\"2019-07-07 14:00:00\",\"wastewater volume\"] - df.loc[\"2019-07-07 14:00:00\", [\"ww line 1\", \"ww line 2\", \"ww line 3\"]].sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbc0a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the sum of the components\n",
    "components_sum = df[cols].sum(axis=1)\n",
    "\n",
    "# Boolean mask where the total is NOT equal to the sum of components\n",
    "mask = ~np.isclose(df[total_col], components_sum, atol=1e-1, rtol=1e-1)\n",
    "\n",
    "# Extract only the inconsistent rows\n",
    "mismatched_rows = df.loc[mask, df.columns].copy()\n",
    "\n",
    "# Add a difference column for clarity\n",
    "mismatched_rows['diff'] = df.loc[mask, total_col] - components_sum[mask]\n",
    "\n",
    "# Display results\n",
    "print(f\"Number of inconsistent rows: {mask.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bd405c",
   "metadata": {},
   "source": [
    "<a id=\"LightGBM\"></a>\n",
    "# <p style=\"color:#E71B24; font-family:'Futura', 'Century Gothic', 'Verdana', sans-serif; font-weight:600; letter-spacing:0.5px; text-align:center;\">PART IV - A First Simple Model, LightGBM</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c74c33",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color:rgb(36, 117, 179); font-size:120%; text-align:left\">\n",
    "\n",
    "To begin our predictive analysis of wastewater volume, we are going to useLightGBM (Light Gradient Boosting Machine). There are several reasons for this choice  :\n",
    "\n",
    "\n",
    "**Simplicity and Speed**: LightGBM is an efficient and fast boosting algorithm, ideal for an initial exploratory model. It allows us to quickly obtain basic results without requiring complex configuration.  \n",
    "\n",
    "**Data Management**: It is capable of handling tabular data with exogenous variables (here the meteorological data), which is relevant for our dataset.  \n",
    "\n",
    "**Robustness**: LightGBM belongs to the decision tree family so it is robust to outliers and can capture nonlinear relationships, which is useful for time series with significant variations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb94252e",
   "metadata": {},
   "source": [
    "<a id=\"FE_LGBM\"></a>\n",
    "## <p style=\"color: rgb(300, 110, 110); font-family:'Futura', 'Century Gothic', 'Verdana', sans-serif; font-weight:600; letter-spacing:0.5px;\"> Part IV.1 Feature Enginerring for LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c4635f",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color:rgb(36, 117, 179); font-size:120%; text-align:left\">\n",
    "\n",
    "LightGBM is supposed to be robust to outliers, so we are not going to adress them for this first model.   \n",
    "With all that we have seen before, every variable in the dataset has a strong temporal dependancy, so lags are going to be key. We are going to build new variables to best represent these previous observations. \n",
    "As the number of variables is not that important for this type of model (at least before 100 variables), we are going to keep all of them for the first try.\n",
    "\n",
    "For every new model, I am going to create a new DataFrame dedicated to it, to keep in memory the old one and adapt the new one best to the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216d5d3d",
   "metadata": {},
   "source": [
    "<a id=\"new_dataframe\"></a>\n",
    "### <p style=\"color: rgb(250, 150, 150); font-family:'Futura', 'Century Gothic', 'Verdana', sans-serif; font-weight:600; letter-spacing:0.5px;\"> Part IV.1.A A New DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf386048",
   "metadata": {},
   "outputs": [],
   "source": [
    "LightGBM_df = df.copy()\n",
    "\n",
    "#changing the name for better handling\n",
    "LightGBM_df = LightGBM_df.rename(columns={\n",
    "    'wastewater volume': 'ww0',\n",
    "    'ww line 1': 'ww1',\n",
    "    'ww line 2': 'ww2',\n",
    "    'ww line 3': 'ww3',\n",
    "    'ww line 4': 'ww4',\n",
    "    'rainfall': 'rainfall_real',\n",
    "    'rain': 'rain_forecast',\n",
    "    'precipitation': 'precipitation_forecast',\n",
    "    'relative_humidity_2m': 'humidity',\n",
    "    'temperature_2m': 'temperature'\n",
    "})\n",
    "\n",
    "LightGBM_df = LightGBM_df.sort_index()  # assure chronological order\n",
    "\n",
    "\n",
    "LightGBM_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec2761d",
   "metadata": {},
   "source": [
    "<a id=\"lags\"></a>\n",
    "### <p style=\"color: rgb(250, 150, 150); font-family:'Futura', 'Century Gothic', 'Verdana', sans-serif; font-weight:600; letter-spacing:0.5px;\">Part IV.1.B Lags & rolling_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "4f3dd7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the last 24 values of ww0 are not know when prediciting batches of 24 hours, so we will remove them\n",
    "LightGBM_df['rainfall_real_lag24'] = LightGBM_df['rainfall_real'].shift(24)\n",
    "\n",
    "# Creating lag features for ww0 to ww4 from lag 24 to lag 48\n",
    "for lag in range(24, 49):\n",
    "    LightGBM_df[f'ww0_lag{lag}'] = LightGBM_df['ww0'].shift(lag)\n",
    "    LightGBM_df[f'ww1_lag{lag}'] = LightGBM_df['ww1'].shift(lag)\n",
    "    LightGBM_df[f'ww2_lag{lag}'] = LightGBM_df['ww2'].shift(lag)\n",
    "    LightGBM_df[f'ww3_lag{lag}'] = LightGBM_df['ww3'].shift(lag)\n",
    "    LightGBM_df[f'ww4_lag{lag}'] = LightGBM_df['ww4'].shift(lag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "5ce4106e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#As observed in the daily correlation matrix, some variables have a significant correlation with the wastewater volume on a daily basis.\n",
    "\n",
    "# Rolling means (6h, 12h, 24h)\n",
    "LightGBM_df['ww0_roll6'] = LightGBM_df['ww0_lag24'].rolling(window=6).mean()\n",
    "LightGBM_df['ww0_roll12'] = LightGBM_df['ww0_lag24'].rolling(window=12).mean()\n",
    "LightGBM_df['ww0_roll24'] = LightGBM_df['ww0_lag24'].rolling(window=24).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314438a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "LightGBM_df = LightGBM_df.dropna()\n",
    "\n",
    "LightGBM_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440046be",
   "metadata": {},
   "source": [
    "<a id=\"LGBM\"></a>\n",
    "## <p style=\"color: rgb(300, 110, 110); font-family:'Futura', 'Century Gothic', 'Verdana', sans-serif; font-weight:600; letter-spacing:0.5px;\"> Part IV.2 Implementing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e0da69",
   "metadata": {},
   "source": [
    "<a id=\"target\"></a>\n",
    "### <p style=\"color: rgb(250, 150, 150); font-family:'Futura', 'Century Gothic', 'Verdana', sans-serif; font-weight:600; letter-spacing:0.5px;\">Part IV.2.A Defining the target and feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a46b400",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color:rgb(36, 117, 179); font-size:120%; text-align:left\">\n",
    "We are here only focusing on the prediction of ww0, as the sum of the four others, and not of each of the ww lines separatly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "be77df94",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'ww0'\n",
    "\n",
    "# we are not using the www lines as features directly, only their lags (otherwise the model is cheating)\n",
    "features = [\n",
    "    # Lags des ww lines et de ww0\n",
    "    *[f'ww0_lag{i}' for i in range(25, 49)],\n",
    "    *[f'ww1_lag{i}' for i in range(25, 49)],\n",
    "    *[f'ww2_lag{i}' for i in range(25, 49)],\n",
    "    *[f'ww3_lag{i}' for i in range(25, 49)],\n",
    "    *[f'ww4_lag{i}' for i in range(25, 49)],\n",
    "    # Rolling\n",
    "    'ww0_roll6', 'ww0_roll12', 'ww0_roll24',\n",
    "    # M√©t√©o\n",
    "    'rainfall_real_lag24', 'rain_forecast', 'precipitation_forecast',\n",
    "    'humidity', 'temperature'\n",
    "]\n",
    "\n",
    "X = LightGBM_df[features]\n",
    "y = LightGBM_df[target]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9661baa",
   "metadata": {},
   "source": [
    "<a id=\"split\"></a>\n",
    "### <p style=\"color: rgb(250, 150, 150); font-family:'Futura', 'Century Gothic', 'Verdana', sans-serif; font-weight:600; letter-spacing:0.5px;\">Part IV.2.B Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "a54348a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = int(len(LightGBM_df) * 0.75)\n",
    "X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]\n",
    "y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6c8379",
   "metadata": {},
   "source": [
    "<a id=\"hyperparameters\"></a>\n",
    "### <p style=\"color: rgb(250, 150, 150); font-family:'Futura', 'Century Gothic', 'Verdana', sans-serif; font-weight:600; letter-spacing:0.5px;\">Part IV.1.C Finding the hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62924ce7",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color:rgb(36, 117, 179); font-size:120%; text-align:left\">\n",
    "\n",
    "To enhance the performance of our LightGBM model, we utilize Optuna, an automated hyperparameter optimization framework. \n",
    "\n",
    "We define a search space for key LightGBM hyperparameters, such as learning_rate, num_leaves, max_depth, and n_estimators. Optuna then searches for the combination of hyperparameters that minimizes the validation error, leading to an optimized LightGBM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26031b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 200),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 200, 1000),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'random_state': 42\n",
    "    }\n",
    "\n",
    "    model = LGBMRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "study.optimize(objective, n_trials=50)\n",
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07ab840",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>\n",
    "### <p style=\"color: rgb(250, 150, 150); font-family:'Futura', 'Century Gothic', 'Verdana', sans-serif; font-weight:600; letter-spacing:0.5px;\">Part IV.1.D Model and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78920955",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Avec Optina\n",
    "model = LGBMRegressor(**study.best_params, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee8860e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"MAE = {mae:.2f}\")\n",
    "print(f\"MSE = {mse:.2f}\")\n",
    "print(f\"R¬≤  = {r2:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894a27f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(y_test.index[:500], y_test[:500], label='True', alpha=0.8)\n",
    "plt.plot(y_test.index[:500], y_pred[:500], label='Predicted', alpha=0.8)\n",
    "plt.title(\"LightGBM forecast ‚Äî 24h ahead (first 500h of test set)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f195689b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(y_test.index[500:1000], y_test[500:1000], label='True', alpha=0.8)\n",
    "plt.plot(y_test.index[500:1000], y_pred[500:1000], label='Predicted', alpha=0.8)\n",
    "plt.title(\"LightGBM forecast ‚Äî 24h ahead (following 500h of test set)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff0524d",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color:rgb(36, 117, 179); font-size:120%; text-align:left\">\n",
    "\n",
    "We can see that the model learned very successfully the most common values (small oscillations around the mean value). \n",
    "\n",
    "However it did not at all predict the peaks, which are likely to be the ones really important to predict.\n",
    "\n",
    "To find areas for improvement, we will look at feature importance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bc9748",
   "metadata": {},
   "source": [
    "<a id=\"feature\"></a>\n",
    "### <p style=\"color: rgb(250, 150, 150); font-family:'Futura', 'Century Gothic', 'Verdana', sans-serif; font-weight:600; letter-spacing:0.5px;\">Part IV.1.E Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf642cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.Series(model.feature_importances_, index=features).sort_values(ascending=False)\n",
    "plt.figure(figsize=(8,8))\n",
    "importances.head(50).plot(kind='barh')\n",
    "plt.title(\"Top 50 Feature Importances ‚Äî LightGBM\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97fae23",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color:rgb(36, 117, 179); font-size:120%; text-align:left\">\n",
    "\n",
    "To improve the performance and efficiency on the **LightGBM model**, we will be implementing several key optimizations and feature engineering strategies : \n",
    "\n",
    "**Focus on Important Features and Reduction of Lags**:  \n",
    "- We will prioritize the use of **features identified as most important** from the feature importance analysis. This ensures that we focus on the variables that have the highest impact on predicting wastewater volume.\n",
    "- Only few lags are very important (as shown in the feature importance analysis), so to simplify the model and reduce computational complexity, we will be using **fewer lags**. This helps in making the model lighter and faster while retaining its predictive power.\n",
    "\n",
    "**Enhanced Feature Engineering**:\n",
    "- We will have **additional feature engineering** on key meteorological variables such as **rainfall, rain forecast, humidity, and temperature**. (which are shown to be important).\n",
    "\n",
    "**Incorporation of Autocorrelation Data**:\n",
    "- We will also add **autocorrelation features**, which we did not really used in the previous version, to better capture the temporal dependencies in the data. These features help the model better understand the patterns and trends in the time series data, leading to more accurate predictions\n",
    "\n",
    "By combining hyperparameter optimization with thoughtful feature engineering, there is good chance to believe the next version will succed better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0daf6e",
   "metadata": {},
   "source": [
    "<a id=\"v2\"></a>\n",
    "# <p style=\"color:#E71B24; font-family:'Futura', 'Century Gothic', 'Verdana', sans-serif; font-weight:600; letter-spacing:0.5px; text-align:center;\">PART V - Improving the LightGBM</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6318050a",
   "metadata": {},
   "source": [
    "<a id=\"FE_LGBM\"></a>\n",
    "## <p style=\"color: rgb(300, 110, 110); font-family:'Futura', 'Century Gothic', 'Verdana', sans-serif; font-weight:600; letter-spacing:0.5px;\"> Part V.1 Feature Enginerring for LightGBM v.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877756ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "LightGBM_v2_df = df.copy()\n",
    "LightGBM_v2_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ae46b8",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>\n",
    "### <p style=\"color: rgb(250, 150, 150); font-family:'Futura', 'Century Gothic', 'Verdana', sans-serif; font-weight:600; letter-spacing:0.5px;\">Part V.1.A Target and lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "9c704b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = \"wastewater volume\"\n",
    "\n",
    "#Shift ww to prevent data leakage\n",
    "LightGBM_v2_df[\"target_col_24\"] = LightGBM_v2_df[target_col].shift(24)\n",
    "LightGBM_v2_df[\"ww1_24\"] = LightGBM_v2_df[\"ww line 1\"].shift(24)\n",
    "LightGBM_v2_df[\"ww2_24\"] = LightGBM_v2_df[\"ww line 2\"].shift(24)\n",
    "LightGBM_v2_df[\"ww3_24\"] = LightGBM_v2_df[\"ww line 3\"].shift(24)\n",
    "LightGBM_v2_df[\"ww4_24\"] = LightGBM_v2_df[\"ww line 4\"].shift(24)\n",
    "\n",
    "# Create lag features (we keep only the most relevant lags based on ACF/PACF and feature importance analysis)\n",
    "for lag in [1, 2, 3, 6, 12, 18, 24, 36, 48]:\n",
    "    LightGBM_v2_df[f\"ww0_lag{lag}\"] = LightGBM_v2_df[\"target_col_24\"].shift(lag)\n",
    "    LightGBM_v2_df[f\"ww1_lag{lag}\"] = LightGBM_v2_df[\"ww1_24\"].shift(lag)\n",
    "    LightGBM_v2_df[f\"ww2_lag{lag}\"] = LightGBM_v2_df[\"ww2_24\"].shift(lag)\n",
    "    LightGBM_v2_df[f\"ww3_lag{lag}\"] = LightGBM_v2_df[\"ww3_24\"].shift(lag)\n",
    "    LightGBM_v2_df[f\"ww4_lag{lag}\"] = LightGBM_v2_df[\"ww4_24\"].shift(lag)\n",
    "\n",
    "# Rolling window averages to capture recent trends\n",
    "for window in [6, 12, 24]:\n",
    "    LightGBM_v2_df[f\"ww0_roll{window}\"] = (\n",
    "        LightGBM_v2_df[\"target_col_24\"].rolling(window=window).mean()\n",
    "    )\n",
    "    \n",
    "# Rolling window averages for ww lines\n",
    "for window in [6, 12, 24]:\n",
    "    LightGBM_v2_df[f\"humidity_roll{window}\"] = (\n",
    "        LightGBM_v2_df[\"relative_humidity_2m\"].rolling(window=window).mean()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "5f89fb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift rainfall to prevent data leakage\n",
    "LightGBM_v2_df[\"rainfall_lag24\"] = LightGBM_v2_df[\"rainfall\"].shift(24)\n",
    "LightGBM_v2_df = LightGBM_v2_df.drop(columns=[\"rainfall\"])\n",
    "\n",
    "# Cumulative rainfall over different periods   \n",
    "LightGBM_v2_df['rain_cum_6h']  = LightGBM_v2_df['rainfall_lag24'].rolling(6).sum()\n",
    "LightGBM_v2_df['rain_cum_12h'] = LightGBM_v2_df['rainfall_lag24'].rolling(12).sum()\n",
    "LightGBM_v2_df['rain_cum_24h'] = LightGBM_v2_df['rainfall_lag24'].rolling(24).sum()\n",
    "\n",
    "# Cumulative rain over different periods\n",
    "LightGBM_v2_df['rain_cum_6h']  = LightGBM_v2_df['rain'].rolling(6).sum()\n",
    "LightGBM_v2_df['rain_cum_12h'] = LightGBM_v2_df['rain'].rolling(12).sum()\n",
    "LightGBM_v2_df['rain_cum_24h'] = LightGBM_v2_df['rain'].rolling(24).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e462512b",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>\n",
    "### <p style=\"color: rgb(250, 150, 150); font-family:'Futura', 'Century Gothic', 'Verdana', sans-serif; font-weight:600; letter-spacing:0.5px;\">Part V.1.B Autocorrelation based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "8d7048f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference compared to previous day\n",
    "LightGBM_v2_df[\"ww0_diff24\"] = (\n",
    "    LightGBM_v2_df[\"target_col_24\"] - LightGBM_v2_df[\"target_col_24\"].shift(24)\n",
    ")\n",
    "\n",
    "# Ratio to previous day (stability / abnormal variation)\n",
    "LightGBM_v2_df[\"ww0_ratio24\"] = (\n",
    "    LightGBM_v2_df[\"target_col_24\"] / (LightGBM_v2_df[\"target_col_24\"].shift(24) + 1e-6)\n",
    ")\n",
    "\n",
    "# Local variation (standard deviation over 12 hours)\n",
    "LightGBM_v2_df[\"ww0_std12\"] = (\n",
    "    LightGBM_v2_df[\"target_col_24\"].rolling(window=12).std()\n",
    ")\n",
    "\n",
    "# 5. Drop rows with missing values (caused by shifts)\n",
    "LightGBM_v2_df = LightGBM_v2_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531b84a4",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>\n",
    "### <p style=\"color: rgb(250, 150, 150); font-family:'Futura', 'Century Gothic', 'Verdana', sans-serif; font-weight:600; letter-spacing:0.5px;\">Part V.1.C temporal / cyclical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022646e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cyclical encoding for hour and day of week\n",
    "LightGBM_v2_df[\"hour_sin\"] = np.sin(2 * np.pi * LightGBM_v2_df[\"hour\"] / 24)\n",
    "LightGBM_v2_df[\"hour_cos\"] = np.cos(2 * np.pi * LightGBM_v2_df[\"hour\"] / 24)\n",
    "LightGBM_v2_df[\"dow_sin\"] = np.sin(2 * np.pi * LightGBM_v2_df[\"dayofweek\"] / 7)\n",
    "LightGBM_v2_df[\"dow_cos\"] = np.cos(2 * np.pi * LightGBM_v2_df[\"dayofweek\"] / 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb51d54",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color:rgb(36, 117, 179); font-size:120%; text-align:left\">\n",
    "\n",
    "**hour_sin and hour_cos:** These features use sine and cosine transformations to encode the hour of the day in a cyclical manner. This helps the model understand that hours are part of a continuous cycle (24 hours in a day), avoiding the issue of treating 23:00 and 00:00 as completely different values.\n",
    "dow_sin and dow_cos: Similarly, these features use sine and cosine transformations to encode the day of the week in a cyclical manner.\n",
    "\n",
    "This helps the model understand that days of the week are part of a continuous cycle (7 days in a week), avoiding the issue of treating Sunday and Monday as completely different values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdb266e",
   "metadata": {},
   "source": [
    "<a id=\"LGBM2\"></a>\n",
    "## <p style=\"color: rgb(300, 110, 110); font-family:'Futura', 'Century Gothic', 'Verdana', sans-serif; font-weight:600; letter-spacing:0.5px;\"> Part V.2 Implementing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398f14a4",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>\n",
    "### <p style=\"color: rgb(250, 150, 150); font-family:'Futura', 'Century Gothic', 'Verdana', sans-serif; font-weight:600; letter-spacing:0.5px;\">Part V.2.A Train/test split by shuffled 7-day blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e48ed6a",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color:rgb(36, 117, 179); font-size:120%; text-align:left\">\n",
    "\n",
    "To ensure a robust evaluation of our model while preserving the temporal structure of the data, we implement a block-based train-test split:\n",
    "\n",
    "\n",
    "**Block Creation:** : We create blocks of data where each block represents a 7-day period. This is done by calculating the number of days since the minimum date in the index and dividing by 7. This ensures that each block contains a full week of data.\n",
    "\n",
    "\n",
    "\n",
    "**Random Shuffling of Blocks:** We shuffle the unique block IDs randomly to distribute the blocks across the train and test sets. This randomness helps in reducing bias and ensures that the model is evaluated on a representative sample of the data.\n",
    "\n",
    "\n",
    "\n",
    "**Train-Test Split:** We allocate 80% of the blocks to the training set and 20% of the blocks to the test set. This ensures that the temporal structure of the data is preserved within each set, which is crucial for time series analysis.\n",
    "train_blocks: The blocks assigned to the training set.\n",
    "test_blocks: The blocks assigned to the test set.\n",
    "\n",
    "\n",
    "\n",
    "**Data Segregation:** We filter the original DataFrame to create separate training and testing DataFrames based on the block IDs.\n",
    "train_df: The DataFrame containing the training data.\n",
    "test_df: The DataFrame containing the test data.\n",
    "\n",
    "\n",
    "\n",
    "This block-based approach ensures that the temporal dependencies within the data are respected, providing a more realistic evaluation of the model's performance on unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e9ea1c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a block index (1 block = 7 days)\n",
    "LightGBM_v2_df[\"block_id\"] = (\n",
    "    (LightGBM_v2_df.index - LightGBM_v2_df.index.min()).days // 7\n",
    ")\n",
    "\n",
    "# Shuffle block IDs randomly\n",
    "unique_blocks = LightGBM_v2_df[\"block_id\"].unique()\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(unique_blocks)\n",
    "\n",
    "# Define 80% train, 20% test by block\n",
    "n_train_blocks = int(len(unique_blocks) * 0.8)\n",
    "train_blocks = unique_blocks[:n_train_blocks]\n",
    "test_blocks = unique_blocks[n_train_blocks:]\n",
    "\n",
    "train_df = LightGBM_v2_df[LightGBM_v2_df[\"block_id\"].isin(train_blocks)]\n",
    "test_df = LightGBM_v2_df[LightGBM_v2_df[\"block_id\"].isin(test_blocks)]\n",
    "\n",
    "# Drop helper column\n",
    "train_df = train_df.drop(columns=[\"block_id\"])\n",
    "test_df = test_df.drop(columns=[\"block_id\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d872aa3",
   "metadata": {},
   "source": [
    "<a id=\"split\"></a>\n",
    "### <p style=\"color: rgb(250, 150, 150); font-family:'Futura', 'Century Gothic', 'Verdana', sans-serif; font-weight:600; letter-spacing:0.5px;\">Part V.2.B Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3dcebf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    "    c for c in LightGBM_v2_df.columns\n",
    "    if c not in [target_col, \"ww line 1\", \"ww line 2\", \"ww line 3\", \"ww line 4\",\"block_id\"]\n",
    "]\n",
    "\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df[target_col]\n",
    "X_test = test_df[feature_cols]\n",
    "y_test = test_df[target_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0899b20e",
   "metadata": {},
   "source": [
    "<a id=\"split\"></a>\n",
    "### <p style=\"color: rgb(250, 150, 150); font-family:'Futura', 'Century Gothic', 'Verdana', sans-serif; font-weight:600; letter-spacing:0.5px;\">Part V.2.C Finding the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "66e331b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"Objective function for Optuna hyperparameter optimization\"\"\"\n",
    "    params = {\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 200),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.05, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 800),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'verbose': -1,\n",
    "        'random_state': 42\n",
    "    }\n",
    "\n",
    "    model = LGBMRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    preds = model.predict(X_val)\n",
    "    mae = mean_absolute_error(y_val, preds)\n",
    "    return mae\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f2c152",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=40, show_progress_bar=False)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(\" Best parameters found by Optuna:\")\n",
    "for k, v in best_params.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06e9322",
   "metadata": {},
   "source": [
    "<a id=\"split\"></a>\n",
    "### <p style=\"color: rgb(250, 150, 150); font-family:'Futura', 'Century Gothic', 'Verdana', sans-serif; font-weight:600; letter-spacing:0.5px;\">Part V.2.D Model and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10c8688",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = LGBMRegressor(**best_params, random_state=42, verbose=-1)\n",
    "best_model.fit(\n",
    "    np.concatenate([X_train, X_val]),\n",
    "    np.concatenate([y_train, y_val])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e488dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\nModel Evaluation on Test Set:\")\n",
    "print(f\"MAE = {mae:.2f}\")\n",
    "print(f\"MSE = {mse:.2f}\")\n",
    "print(f\"R¬≤  = {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabae391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the prediction\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(y_test.values[:500], label=\"Actual\", linewidth=2)\n",
    "plt.plot(y_pred[:500], label=\"Predicted\", linewidth=2)\n",
    "plt.title(\"Wastewater Volume ‚Äî Predicted vs Actual (first 500 samples)\")\n",
    "plt.xlabel(\"Sample index\")\n",
    "plt.ylabel(\"Wastewater volume\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a257a8b3",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color:rgb(36, 117, 179); font-size:120%; text-align:left\">\n",
    "\n",
    "We can see here that the peaks are detected much more accurately by the model.\n",
    "However, the MAE was lower with the previous model. This one is more ‚Äúreactive‚Äù (or ‚Äúsensitive‚Äù), meaning it tends to oscillate more around small fluctuations, which reduces its point-by-point precision.\n",
    "But because it captures large variations and extreme values better, its overall R¬≤ score is higher.\n",
    "\n",
    "Overall, I htink this model is likely more useful for real-world forecasting, as accurately anticipating major peaks is probably more valuable than achieving very high precision on minor fluctuations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c17236",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "# <p style=\"color:#E71B24; font-family:'Futura', 'Century Gothic', 'Verdana', sans-serif; font-weight:600; letter-spacing:0.5px; text-align:center;\">PART VII - Testing a more advanced model using Deep Learning - Ts2Vec</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebb3f61",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color:rgb(36, 117, 179); font-size:120%; text-align:left\">\n",
    "\n",
    "In this final step, we will examine a more advanced model that directly addresses a challenge we identified earlier: the presence of multiple phenomena evolving across different temporal scales.  \n",
    "\n",
    "Since the data contains both short-term fluctuations and long-term structural patterns, the key objective is to learn a **contextual, multi-scale temporal representation** of the features. This also naturally supports the **multivariate nature** of the problem (ww1 + ww2 + ww3 + ww4), allowing the model to encode how variables interact and evolve together over time.\n",
    "\n",
    "What makes this approach particularly appealing is its ability to capture **complex, evolving dynamics** at several temporal resolutions simultaneously. The model is designed to encode both local variations and global trends, providing a richer and more expressive understanding of temporal behavior than traditional architectures.\n",
    "\n",
    "The idea comes from this paper [https://arxiv.org/abs/2106.10466](https://arxiv.org/abs/2106.10466) ‚Äî which provides both the theoretical foundations and a ready-to-use implementation. Given its relevance and the availability of the code, testing this model became a compelling choice for our experiments.\n",
    "\n",
    "Moreover, the original paper describes not only how to build such representations, but also how to use them for robust forecasting tasks. Across many datasets, the proposed method achieves **state-of-the-art results**, demonstrating strong generalization and predictive power.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954738e5",
   "metadata": {},
   "source": [
    "<a id=\"FE_LGBM\"></a>\n",
    "## <p style=\"color: rgb(300, 110, 110); font-family:'Futura', 'Century Gothic', 'Verdana', sans-serif; font-weight:600; letter-spacing:0.5px;\"> Part VII.1 Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "6924aa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts2vec_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "1140d18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample df into daily_df taking 24 entry and averaging them into one daily entry\n",
    "ts2vec_df.index.name = 'date'               # give the index a name so reset_index yields 'date'\n",
    "daily_df = ts2vec_df.resample('D').mean()\n",
    "daily_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4193d945",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fd4ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train and test set with 2/3 and 1/3 of the data\n",
    "n = len(ts2vec_df)\n",
    "train_size = int(n * (2/3))\n",
    "train_df = ts2vec_df[:train_size]\n",
    "test_df = ts2vec_df[train_size:]\n",
    "\n",
    "print(f\"Train set shape: {train_df.shape}\")\n",
    "print(f\"Test set shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe38565",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "271b3104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove \"wastewater volume\" from train and test set\n",
    "train_df = train_df.drop(columns=['wastewater volume'])\n",
    "test_df = test_df.drop(columns=['wastewater volume'])\n",
    "\n",
    "\n",
    "# add a dimesion to train and test set to make them 3D (samples, timesteps, features)\n",
    "train_data = np.expand_dims(train_df.values, axis=0)\n",
    "test_data = np.expand_dims(test_df.values, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b91591e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train_data shape:\", np.array(train_data).shape)\n",
    "print(\"test_data shape:\", np.array(test_data).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4caafa",
   "metadata": {},
   "source": [
    "<a id=\"FE_LGBM\"></a>\n",
    "## <p style=\"color: rgb(300, 110, 110); font-family:'Futura', 'Century Gothic', 'Verdana', sans-serif; font-weight:600; letter-spacing:0.5px;\"> Part VII.2 Implementing the model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712118ab",
   "metadata": {},
   "source": [
    "<a id=\"new_dataframe\"></a>\n",
    "### <p style=\"color: rgb(250, 150, 150); font-family:'Futura', 'Century Gothic', 'Verdana', sans-serif; font-weight:600; letter-spacing:0.5px;\"> Part VII.2.A Training ts2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73566f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train TS2Vec model on train set\n",
    "\n",
    "\n",
    "model = TS2Vec(\n",
    "    input_dims=train_data.shape[2],\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    max_train_length=500\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15b6074",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_log = model.fit(\n",
    "    train_data,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8552205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_repr = model.encode(train_data,causal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac31ae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_repr = model.encode(test_data,causal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873706ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_repr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1fcc3f",
   "metadata": {},
   "source": [
    "<a id=\"new_dataframe\"></a>\n",
    "### <p style=\"color: rgb(250, 150, 150); font-family:'Futura', 'Century Gothic', 'Verdana', sans-serif; font-weight:600; letter-spacing:0.5px;\"> Part VII.2.B Linear regression model on the encoded train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b190961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are going to train a simple linear regression model on the test set repr to predict the 4 ww lines\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "post_len=24\n",
    "\n",
    "for i in range(0, train_repr.shape[1] - post_len,1):\n",
    "    X_train.append(np.concatenate((train_repr[0, i].flatten(), train_df.iloc[i:i + post_len][[\"temperature_2m\",\"rain\",\"relative_humidity_2m\"]].values.flatten())))\n",
    "    y_train.append(train_df.iloc[i:i + post_len][['ww line 1', 'ww line 2', 'ww line 3', 'ww line 4']].values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d360d0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b36096b",
   "metadata": {},
   "source": [
    "<a id=\"new_dataframe\"></a>\n",
    "### <p style=\"color: rgb(250, 150, 150); font-family:'Futura', 'Century Gothic', 'Verdana', sans-serif; font-weight:600; letter-spacing:0.5px;\"> Part VII.2.C Evalution of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a39a2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on test set\n",
    "X_test = []\n",
    "y_test = []\n",
    "for i in range(1, test_repr.shape[1] - post_len,25):\n",
    "    X_test.append(np.concatenate((test_repr[0, i].flatten(), test_df.iloc[i:i + post_len][[\"temperature_2m\",\"rain\",\"relative_humidity_2m\"]].values.flatten())))\n",
    "    y_test.append(test_df.iloc[i:i + post_len][['ww line 1', 'ww line 2', 'ww line 3', 'ww line 4']].values.flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe8a227",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = reg.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Test MSE: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0f28d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"y_test shape:\", np.array(y_test).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5859ac74",
   "metadata": {},
   "source": [
    "<a id=\"new_dataframe\"></a>\n",
    "### <p style=\"color: rgb(250, 150, 150); font-family:'Futura', 'Century Gothic', 'Verdana', sans-serif; font-weight:600; letter-spacing:0.5px;\"> Part VII.2.D Recreating the wastewater volume feature to plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5a6979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reacreate the feature wastewater (juste divide the 96 values in 4 clusters and sum them)\n",
    "y_test_sum = np.sum(np.array(y_test).reshape(-1, 24, 4), axis=2)\n",
    "y_pred_sum = np.sum(y_pred.reshape(-1, 24, 4), axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c9feb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute mse on the sum,R2score,MAE\n",
    "mse_sum = mean_squared_error(y_test_sum, y_pred_sum)\n",
    "r2 = r2_score(y_test_sum, y_pred_sum)\n",
    "mae = mean_absolute_error(y_test_sum, y_pred_sum)\n",
    "print(f\"Test MSE on sum: {mse_sum}\")\n",
    "print(f\"Test R2 score on sum: {r2}\")\n",
    "print(f\"Test MAE on sum: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c5cfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shape of y_test_sum\n",
    "print(\"y_test_sum shape:\", y_test_sum.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2e297e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare true vs predicted sum over test set\n",
    "N=2\n",
    "for i in range(N):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(y_test_sum[i], label='True Sum')\n",
    "    plt.plot(y_pred_sum[i], label='Predicted Sum')\n",
    "    plt.title(f'Comparison of True vs Predicted Wastewater Volume Sum - Sample {i+1}')\n",
    "    plt.xlabel('Hour')\n",
    "    plt.ylabel('Wastewater Volume Sum')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692a4f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.concatenate(y_pred_sum, axis=0)\n",
    "y_test = np.concatenate(y_test_sum, axis=0)\n",
    "\n",
    "N=250\n",
    "i=50\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(y_test[i:i+N], label='True Sum', alpha=0.7)\n",
    "plt.plot(y_pred[i:i+N], label='Predicted Sum', alpha=0.7)\n",
    "plt.title('Comparison of True vs Predicted Wastewater Volume Sum')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Wastewater Volume Sum')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf8bfce",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color:rgb(36, 117, 179); font-size:120%; text-align:left\">\n",
    "In conclusion, this project allowed me to familiarize myself with an example of what an industrial prediction project at Veolia might look like. \n",
    "\n",
    "This dataset was very rich and therefore particularly interesting to explore and try to understand in depth. It also allowed me to do some research and discover new models. However, there are still several avenues that I did not have time to explore sufficiently, such as ARIMA statistical models, which are widely used in time series but which I found complicated to configure in order to understand a problem of this complexity (requiring a multi-scale temporal approach). \n",
    "\n",
    "Finally, in a project of this type, compromises may have to be made (such as choosing between a model that is more accurate for small variations or one that better detects peaks, see light GBM 1 and 2), and in this case, regular contact with the project's sponsoring teams would be particularly useful, which was not possible in this project.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
